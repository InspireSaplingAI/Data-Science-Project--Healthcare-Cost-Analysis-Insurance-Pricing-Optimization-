{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis with PCA\n",
    "\n",
    "This notebook analyzes feature importance using 4 standard methods plus PCA to understand the underlying structure of medical cost drivers.\n",
    "\n",
    "It is used to select most important features and determine threshold to identiy high risk users (used in src/high_risk_identification.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "import statsmodels.api as sm\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv('../data/insurance_cleaned.csv')\n",
    "\n",
    "# Convert categorical variables\n",
    "df['smoker'] = df['smoker'].map({'yes': 1, 'no': 0})\n",
    "df = pd.get_dummies(df, columns=['sex', 'region'], drop_first=True)\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop('charges', axis=1)\n",
    "y = df['charges']\n",
    "feature_names = X.columns\n",
    "\n",
    "# Standardize for PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Analysis\n",
    "PCA identifies key patterns in the data by transforming features into uncorrelated principal components ordered by their variance contribution. For insurance pricing, PCA reveals hidden relationships between risk factors like age, BMI, and smoking that jointly impact costs. It validates if manual thresholds align with actual high-cost clusters in the multidimensional feature space.\n",
    "\n",
    "Also useful for dimention reduction if too many features (Project original features onto the principal components to reduce dimensions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_)+1), \n",
    "         pca.explained_variance_ratio_.cumsum(), \n",
    "         marker='o', linestyle='--')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '95% cut-off threshold', color='red', fontsize=16)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Create PCA loadings heatmap\n",
    "loadings = pd.DataFrame(pca.components_.T,\n",
    "                       columns=[f'PC{i}' for i in range(1, len(feature_names)+1)],\n",
    "                       index=feature_names)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(loadings.iloc[:, :3],  # First 3 principal components\n",
    "            annot=True, \n",
    "            cmap='Spectral', \n",
    "            center=0,\n",
    "            fmt=\".2f\")\n",
    "plt.title('PCA Component Loadings (First 3 Components)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Loadings Heatmap Interpretation\n",
    "\n",
    "#### 1. Feature Importance in Principal Components\n",
    "- The color intensity shows each feature's contribution to PCs\n",
    "- Strong positive values (closer to +1, red): Feature strongly positively influences the PC\n",
    "- Strong negative values (closer to -1, blue): Feature strongly negatively influences the PC\n",
    "- Values near zero (white/yellow): Feature has minimal impact on the PC\n",
    "\n",
    "#### 2. Correlation Patterns Revealed\n",
    "- Features with same-sign loadings on a PC are positively correlated\n",
    "- Features with opposite-sign loadings on a PC are negatively correlated\n",
    "- This helps identify groups of features that move together or in opposition\n",
    "\n",
    "#### 3. Key Variance Drivers\n",
    "- PC1 (first column) shows which features account for most variance\n",
    "- Subsequent PCs reveal secondary patterns in the data\n",
    "- High absolute values indicate dominant features for each pattern\n",
    "\n",
    "#### 4. Actionable Insights\n",
    "- Features with strong loadings in top PCs should be prioritized\n",
    "- Features with near-zero loadings across PCs may be candidates for removal\n",
    "- The heatmap helps validate whether manual thresholds align with actual data patterns\n",
    "\n",
    "#### Example Interpretation\n",
    "If the heatmap shows:\n",
    "- smoker: 0.85 (PC1), -0.1 (PC2), 0.2 (PC3)\n",
    "- age: 0.6 (PC1), 0.4 (PC2), 0.1 (PC3)\n",
    "\n",
    "This indicates:\n",
    "1. Smoking status is the strongest driver of PC1\n",
    "2. Age contributes substantially to both PC1 and PC2\n",
    "3. These two features likely have the biggest impact on cost variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create biplot for PC1 vs PC2\n",
    "def pca_biplot(score, coeff, labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(xs * scalex, ys * scaley, c=y, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(label='Medical Charges')\n",
    "    \n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color='r', alpha=0.5)\n",
    "        plt.text(coeff[i,0]*1.15, coeff[i,1]*1.15, labels[i], color='r', ha='center', va='center')\n",
    "    \n",
    "    plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "    plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "    plt.title('PCA Biplot')\n",
    "    plt.grid()\n",
    "\n",
    "pca_biplot(X_pca[:,:2], np.transpose(pca.components_[:2, :]), labels=feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from PC1 vs PC2 Biplot\n",
    "\n",
    "#### 1. Variance Explanation\n",
    "- PC1 (x-axis): Accounts for [X]% of total variance (primary pattern)\n",
    "- PC2 (y-axis): Accounts for [Y]% of total variance (secondary pattern)\n",
    "\n",
    "#### 2. Feature Relationships\n",
    "- Arrow direction shows how original features contribute to PCs:\n",
    "  - Right-pointing: Strong positive contribution to PC1\n",
    "  - Left-pointing: Strong negative contribution to PC1\n",
    "  - Up-pointing: Strong positive contribution to PC2\n",
    "  - Down-pointing: Strong negative contribution to PC2\n",
    "\n",
    "#### 3. Correlation Patterns\n",
    "- Features with arrows in similar directions are positively correlated\n",
    "- Features with arrows in opposite directions are negatively correlated\n",
    "- Perpendicular arrows indicate uncorrelated features\n",
    "\n",
    "#### 4. High-Risk Patient Clusters\n",
    "- Points colored by medical charges (viridis scale) show:\n",
    "  - High-cost patients cluster in [specific quadrant/direction]\n",
    "  - Low-cost patients cluster in [opposite area]\n",
    "\n",
    "#### 5. Dominant Cost Drivers\n",
    "- Longest arrows represent most influential features:\n",
    "  - PC1 typically dominated by [top feature, e.g., smoking status]\n",
    "  - PC2 often reveals [secondary pattern, e.g., age-BMI interaction]\n",
    "\n",
    "#### 6. Practical Applications\n",
    "- Identifies subgroups with similar risk profiles\n",
    "- Validates whether manual thresholds (e.g., age>50) align with natural clusters\n",
    "- Reveals unexpected feature interactions affecting costs\n",
    "\n",
    "#### Example Interpretation\n",
    "If the biplot shows:\n",
    "- Smoker arrow pointing far right along PC1\n",
    "- Age and BMI arrows pointing up-right\n",
    "- Children arrow pointing down-left\n",
    "\n",
    "This suggests:\n",
    "1. Smoking is the primary cost driver (PC1)\n",
    "2. Older patients with higher BMI form a distinct risk group\n",
    "3. Patients with more children tend to have lower costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting PCA Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "1. **Dimensionality Reduction**:\n",
    "   - The first 3 principal components explain XX% of variance\n",
    "   - We could reduce features to 3 components while retaining most information\n",
    "\n",
    "2. **Component Interpretation**:\n",
    "   - **PC1**: Strongly associated with ______ (positive) and ______ (negative)\n",
    "   - **PC2**: Captures ______ pattern\n",
    "   - **PC3**: Represents ______\n",
    "\n",
    "3. **Feature Relationships**:\n",
    "   - Features pointing in same direction are positively correlated\n",
    "   - Features in opposite directions are negatively correlated\n",
    "   - Right-angle features are uncorrelated\n",
    "\n",
    "4. **Charge Patterns**:\n",
    "   - Higher charges tend to cluster in ______ direction of the biplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Feature Importance Methods (Now Informed by PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "# Todo: complete this part and get the feature importance\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "sns.barplot(x='importance', y='feature', data=rf_importance)\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Permutation Importance\n",
    "Permutation importance measures how much a model's performance drops when a feature's values are randomly shuffled. It quantifies how much the model relies on each feature to make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Random Forest for permutation importance\n",
    "result = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "# The key reason that using test data is to avoid overfitting bias and get a true measure of feature importance for generalization. \n",
    "perm_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': result.importances_mean,\n",
    "    'std': result.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "sns.barplot(x='importance', y='feature', data=perm_importance)\n",
    "plt.title('Permutation Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: SHAP Values\n",
    "#### Definition:\n",
    "SHAP values quantify the contribution of each feature to a single prediction (local explanation) or the entire model (global explanation), based on cooperative game theory.\n",
    "\n",
    "#### Key Properties:\n",
    "\n",
    "- Additive: The sum of all feature contributions equals the difference between the model’s prediction and the baseline (average prediction).\n",
    "- Interpretable: Shows how much each feature pushed the prediction higher or lower for a specific case.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get mean absolute SHAP values\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: Statistical Regression \n",
    "In linear regression models (like Ordinary Least Squares (OLS)), the coefficients represent:\n",
    "\n",
    "- Direction: Positive/Negative sign shows if the feature increases or decreases the target.\n",
    "\n",
    "- Magnitude: Larger absolute values mean stronger impact on the target (assuming features are standardized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for comparable coefficients\n",
    "X_standardized = (X - X.mean()) / X.std()\n",
    "X_standardized = sm.add_constant(X_standardized)\n",
    "\n",
    "# Fit OLS model\n",
    "model = sm.OLS(y, X_standardized).fit()\n",
    "\n",
    "# Get standardized coefficients\n",
    "coef_importance = pd.DataFrame({\n",
    "    'feature': ['const'] + list(feature_names),\n",
    "    'importance': model.params,\n",
    "    'p_value': model.pvalues\n",
    "}).sort_values('importance', key=abs, ascending=False)\n",
    "\n",
    "# Plot (excluding intercept)\n",
    "sns.barplot(x='importance', y='feature', \n",
    "            data=coef_importance[coef_importance['feature'] != 'const'])\n",
    "plt.title('Standardized Regression Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Comparative Analysis (Including PCA Insights)\n",
    "\n",
    "### Consensus Importance\n",
    "\n",
    "#### Definition\n",
    "A combined metric that averages normalized feature importance scores from multiple methods to identify the most reliable predictors.\n",
    "\n",
    "#### Methods Combined\n",
    "- RandomForest (Gini importance)\n",
    "- Permutation (performance-based)\n",
    "- SHAP (contribution-based)\n",
    "- Regression (standardized coefficients)\n",
    "\n",
    "#### Calculation Steps\n",
    "1. Normalize each method's importance scores to [0,1] range\n",
    "2. Average the normalized scores across all methods\n",
    "3. Sort features by the averaged \"consensus\" score\n",
    "\n",
    "#### Key Properties\n",
    "1. Robustness\n",
    "   - Reduces bias from any single method\n",
    "   - Only features important across multiple methods rank highly\n",
    "\n",
    "2. Interpretation\n",
    "   - Scores range from 0 (unimportant) to 1 (most important)\n",
    "   - Higher values indicate stronger agreement between methods\n",
    "\n",
    "3. Advantages\n",
    "   - More reliable than single-method importance\n",
    "   - Identifies universally important features\n",
    "   - Filters out method-specific artifacts\n",
    "\n",
    "#### Example Output Format\n",
    "feature | consensus | rf | perm | shap | reg\n",
    "------- | --------- | -- | ---- | ---- | ---\n",
    "smoker | 0.95 | 1.0 | 0.90 | 0.98 | 0.92\n",
    "age | 0.80 | 0.85 | 0.75 | 0.82 | 0.78\n",
    "bmi | 0.60 | 0.70 | 0.55 | 0.65 | 0.50\n",
    "\n",
    "#### Usage Recommendations\n",
    "- Focus on features with consensus > 0.8\n",
    "- Investigate features with high variance between methods\n",
    "- Use for feature selection in final models\n",
    "\n",
    "#### Python Implementation\n",
    "```python\n",
    "# Normalize importance scores (0-1)\n",
    "comparison_norm = comparison.apply(lambda x: (x - x.min())/(x.max()-x.min()))\n",
    "\n",
    "# Calculate consensus\n",
    "comparison_norm['consensus'] = comparison_norm.mean(axis=1)\n",
    "consensus_importance = comparison_norm['consensus'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all importance metrics\n",
    "comparison = pd.DataFrame({\n",
    "    'RandomForest': rf_importance.set_index('feature')['importance'],\n",
    "    'Permutation': perm_importance.set_index('feature')['importance'],\n",
    "    'SHAP': shap_importance.set_index('feature')['importance'],\n",
    "    'Regression': coef_importance.set_index('feature')['importance'].drop('const'),\n",
    "    'PCA_PC1': loadings['PC1'],\n",
    "    'PCA_PC2': loadings['PC2']\n",
    "})\n",
    "\n",
    "# Normalize each method to 0-1 scale for comparison\n",
    "comparison_norm = comparison.apply(lambda x: (x - x.min())/(x.max()-x.min()), axis=0)\n",
    "\n",
    "# Plot\n",
    "comparison_norm.plot(kind='bar', figsize=(14, 7))\n",
    "plt.title('Normalized Feature Importance Comparison (Including PCA Loadings)')\n",
    "plt.ylabel('Normalized Importance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate consensus importance (excluding PCA for thresholds)\n",
    "comparison_norm['consensus'] = comparison_norm[['RandomForest', 'Permutation', 'SHAP', 'Regression']].mean(axis=1)\n",
    "consensus_importance = comparison_norm['consensus'].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nConsensus Feature Importance:\")\n",
    "print(consensus_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA-Informed Threshold Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze PCA component extremes to find high-risk profiles\n",
    "pca_df = pd.DataFrame(X_pca[:, :3], columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_df['charges'] = y.values\n",
    "\n",
    "# Find top 5% extremes in PC1 (most important component)\n",
    "pc1_thresh = pca_df['PC1'].quantile(0.95)\n",
    "high_pc1 = df[pca_df['PC1'] > pc1_thresh]\n",
    "\n",
    "# Profile these high-PC1 cases\n",
    "print(\"\\nCharacteristics of High PC1 Cases (Top 5%):\")\n",
    "print(high_pc1.describe().loc[['mean', 'std']])\n",
    "\n",
    "# Visualize feature distributions in high-PC1 group\n",
    "plt.figure(figsize=(14, 8))\n",
    "# use top features\n",
    "for i, col in enumerate(['age', 'bmi', 'smoker']):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.kdeplot(data=df, x=col, label='All')\n",
    "    sns.kdeplot(data=high_pc1, x=col, label='High PC1')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommend thresholds based on PCA high-risk profile\n",
    "print(\"\\nPCA-Informed Threshold Recommendations:\")\n",
    "print(f\"- Age: >{high_pc1['age'].mean():.1f} (vs overall mean {df['age'].mean():.1f})\")\n",
    "print(f\"- BMI: >{high_pc1['bmi'].mean():.1f} (vs overall mean {df['bmi'].mean():.1f})\")\n",
    "print(f\"- Smoker: {'yes' if high_pc1['smoker'].mean() > 0.5 else 'no'}\")\n",
    "print(f\"- Average charges in this group: ${high_pc1['charges'].mean():,.0f} (vs overall ${df['charges'].mean():,.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final High-Risk Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Recommended high-risk criteria combining all methods\n",
    "high_risk_mask = (\n",
    "    (df['age'] > 50) &          # From PCA and consensus analysis\n",
    "    (df['bmi'] > 35) &          # Slightly higher than PCA suggests for specificity\n",
    "    (df['smoker'] == 1) &       # Universal agreement across methods\n",
    "    (df['children'] < 3)        # From PC2 analysis showing inverse relationship\n",
    ")\n",
    "```\n",
    "\n",
    "**Justification**:\n",
    "- Captures patients with:\n",
    "  - Demographic risk (age)\n",
    "  - Health risk (BMI + smoking)\n",
    "  - Lower dependents (from PC2 loadings)\n",
    "- Represents XX% of population but XX% of total charges\n",
    "- Average charges XX× higher than non-high-risk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
